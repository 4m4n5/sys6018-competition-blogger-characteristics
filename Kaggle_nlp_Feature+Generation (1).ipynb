{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import *\n",
    "from nltk import sent_tokenize\n",
    "import textstat\n",
    "from textstat.textstat import textstat\n",
    "import pyphen\n",
    "pyphen.language_fallback('nl_NL_variant1')\n",
    "'nl_NL'\n",
    "'nl_NL' in pyphen.LANGUAGES\n",
    "True\n",
    "dic = pyphen.Pyphen(lang='nl_NL')\n",
    "from textstat.textstat import textstat\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train dataset\n",
    "train = pd.read_csv('/home/dell/Desktop/MSDS/Fall/data mining (SYS)/kaggle-nlp/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "Dts = []\n",
    "prps = []\n",
    "vbp= []\n",
    "nouns =[]\n",
    "for y in range(0,442961):\n",
    "    s = train['text'][y]\n",
    "    a = s.split()\n",
    "    #print('text size:' + str(len(a)))\n",
    "    list1 = nltk.pos_tag(a)\n",
    "    #print(list1)\n",
    "    pos =[]\n",
    "    count = 0\n",
    "    #print(pos)\n",
    "    for x in list1:\n",
    "    #print(x[1])\n",
    "        pos.append(x[1])\n",
    "\n",
    "#for i in list1:\n",
    " #   pos.append(i[1])\n",
    "    #print(pos)\n",
    "    for i in pos:\n",
    "        if i == 'DT':\n",
    "            count = count + 1\n",
    "        elif i == 'PRP':\n",
    "            count1 = count1 + 1\n",
    "        elif i == 'VBP' or i == 'VBG' or i == 'VB' or i =='VBD' or i == 'VBN' or i == 'VBZ':\n",
    "            count2 = count + 1\n",
    "        elif i == 'NN' or i == 'NNS' or i == 'NNP' or i == 'NNPS':\n",
    "            count3 = count3 + 1\n",
    "    nouns.append(count3)\n",
    "    vbp.append(count2)\n",
    "    prps.append(count1)\n",
    "    Dts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating text statistics\n",
    "numwords = []\n",
    "numsent = []\n",
    "syllable = []\n",
    "avgsyllables = []\n",
    "readability_scores = []\n",
    "fog_level = []\n",
    "smog_level = []\n",
    "coleman_level = []\n",
    "linsear_level = []\n",
    "numdiffwords = []\n",
    "dale_score = []\n",
    "standard_level = []\n",
    "\n",
    "\n",
    "for x in range(0,442961):\n",
    "    s = train['text'][x]\n",
    "    word = textstat.lexicon_count(s)\n",
    "    numwords.append(word)\n",
    "    syl = textstat.syllable_count(s)\n",
    "    syllable.append(syl)\n",
    "    sentcnt = textstat.sentence_count(s)\n",
    "    numsent.append(sentcnt)\n",
    "    smog = textstat.smog_index(s)\n",
    "    smog_level.append(smog)\n",
    "    linsear = textstat.linsear_write_formula(s)\n",
    "    linsear_level.append(linsear)\n",
    "    diffwords = textstat.difficult_words(s)\n",
    "    numdiffwords.append(diffwords)\n",
    "    dale = textstat.dale_chall_readability_score(s)\n",
    "    dale_score.append(dale)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = np.asarray(numwords)\n",
    "x2 = np.asarray(syllable)\n",
    "avgsyllables = np.divide(x2,x1)\n",
    "avgsyllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding data to train data set\n",
    "train['numofnouns'] = nouns\n",
    "train['numofverbs'] = vbp\n",
    "train['numofdeterminers'] = Dts\n",
    "train['numofperspronouns'] = prps\n",
    "train['numwords'] = numwords\n",
    "train['numsentences'] = numsent\n",
    "train['totnumsyllables'] = syl\n",
    "train['avgsyllables'] = avgsyllables\n",
    "train['smog score'] = smog_level\n",
    "train['linsear score'] = linsear_level\n",
    "train['numdifficultwords'] = numdiffwords\n",
    "train['dale score'] = dale_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#group by age\n",
    "grouped4 = train.groupby(['age']).mean()\n",
    "grouped4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make csv file\n",
    "grouped4.to_csv('/home/dell/train_output5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import test dataset\n",
    "test = pd.read_csv('/home/dell/Desktop/MSDS/Fall/data mining (SYS)/kaggle-nlp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "Dts = []\n",
    "prps = []\n",
    "vbp= []\n",
    "nouns =[]\n",
    "for y in range(0,442961):\n",
    "    s = test['text'][y]\n",
    "    a = s.split()\n",
    "    #print('text size:' + str(len(a)))\n",
    "    list1 = nltk.pos_tag(a)\n",
    "    #print(list1)\n",
    "    pos =[]\n",
    "    count = 0\n",
    "    #print(pos)\n",
    "    for x in list1:\n",
    "    #print(x[1])\n",
    "        pos.append(x[1])\n",
    "\n",
    "#for i in list1:\n",
    " #   pos.append(i[1])\n",
    "    #print(pos)\n",
    "    for i in pos:\n",
    "        if i == 'DT':\n",
    "            count = count + 1\n",
    "        elif i == 'PRP':\n",
    "            count1 = count1 + 1\n",
    "        elif i == 'VBP' or i == 'VBG' or i == 'VB' or i =='VBD' or i == 'VBN' or i == 'VBZ':\n",
    "            count2 = count + 1\n",
    "        elif i == 'NN' or i == 'NNS' or i == 'NNP' or i == 'NNPS':\n",
    "            count3 = count3 + 1\n",
    "    nouns.append(count3)\n",
    "    vbp.append(count2)\n",
    "    prps.append(count1)\n",
    "    Dts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test dataset text stats\n",
    "numwords = []\n",
    "numsent = []\n",
    "syllable = []\n",
    "avgsyllables = []\n",
    "readability_scores = []\n",
    "fog_level = []\n",
    "smog_level = []\n",
    "coleman_level = []\n",
    "linsear_level = []\n",
    "numdiffwords = []\n",
    "dale_score = []\n",
    "standard_level = []\n",
    "\n",
    "\n",
    "for x in range(0,442961):\n",
    "    s = test['text'][x]\n",
    "    word = textstat.lexicon_count(s)\n",
    "    numwords.append(word)\n",
    "    syl = textstat.syllable_count(s)\n",
    "    syllable.append(syl)\n",
    "    sentcnt = textstat.sentence_count(s)\n",
    "    numsent.append(sentcnt)\n",
    "    smog = textstat.smog_index(s)\n",
    "    smog_level.append(smog)\n",
    "    linsear = textstat.linsear_write_formula(s)\n",
    "    linsear_level.append(linsear)\n",
    "    diffwords = textstat.difficult_words(s)\n",
    "    numdiffwords.append(diffwords)\n",
    "    dale = textstat.dale_chall_readability_score(s)\n",
    "    dale_score.append(dale)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating avg syllables\n",
    "x1 = np.asarray(numwords)\n",
    "x2 = np.asarray(syllable)\n",
    "avgsyllables = np.divide(x2,x1)\n",
    "avgsyllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding data to test dataset\n",
    "test['numofnouns'] = nouns\n",
    "test['numofverbs'] = vbp\n",
    "test['numofdeterminers'] = Dts\n",
    "test['numofperspronouns'] = prps\n",
    "test['numwords'] = numwords\n",
    "test['numsentences'] = numsent\n",
    "test['totnumsyllables'] = syl\n",
    "test['avgsyllables'] = avgsyllables\n",
    "test['smog score'] = smog_level\n",
    "test['linsear score'] = linsear_level\n",
    "test['numdifficultwords'] = numdiffwords\n",
    "test['dale score'] = dale_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by user id\n",
    "grouped5 = test.groupby(['user.id']).mean()\n",
    "grouped5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert to csv\n",
    "grouped5.to_csv('/home/dell/train_output5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
